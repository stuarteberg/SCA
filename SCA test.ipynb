{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/nrs/flyem/bergs/tmp/fish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:06:54.283023Z",
     "start_time": "2019-08-02T19:06:54.278618Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys, time\n",
    "from collections import Iterable\n",
    "\n",
    "import dask.array as da\n",
    "import dask_jobqueue\n",
    "from dask_jobqueue import LSFCluster\n",
    "from fish.util.distributed import get_jobqueue_cluster\n",
    "from dask.distributed import Client\n",
    "#import B.general_admin as ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:31:15.415756Z",
     "start_time": "2019-08-02T19:31:15.409413Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    ''' hardcoded function to load data (≈6GB) ''' \n",
    "    folder = '/nrs/ahrens/Virginia_nrs/25_LS_wG/190703_HuCH2B_gCaMP7F_8dpf_forG/exp0/sca/'\n",
    "    fname = folder + 'data_ntk_demeaned.npy'\n",
    "    return np.load(fname)\n",
    "\n",
    "# np.save('/nrs/flyem/bergs/tmp/data_ntk_demeaned-c-order.npy', np.asarray(load_data(), order='C'))\n",
    "def load_data_c_order():\n",
    "    data = np.load('/nrs/flyem/bergs/tmp/data_ntk_demeaned-c-order.npy')\n",
    "    return data\n",
    "\n",
    "def construct_adiff(j, data_ntk=None):\n",
    "    \"\"\"\n",
    "    Process a single neuron, as specified by 'j'.\n",
    "    \"\"\"\n",
    "    if data_ntk is None:\n",
    "        data_ntk = load_data_c_order()\n",
    "\n",
    "    n, t, k = data_ntk.shape # n = 31305, t = 50, k ≈ 800 \n",
    "    jtk = data_ntk[j] # load data from neuron j\n",
    "    mat_nt2 = np.zeros([n, t**2], np.float32) # prepare result matrix\n",
    "    for i in range(n): # loop over all neurons...\n",
    "        if np.mod(i, 5000) == 0:\n",
    "            print('n: {0}'.format(i))\n",
    "        itk = data_ntk[i]\n",
    "        ai = itk.dot(jtk.T)/k # construct TxT covariance matrix      \n",
    "        ai_diff = (ai - ai.T).reshape([1, t**2]) # construct small cdiff and reshape into 1x(T^2)\n",
    "        mat_nt2[i] = ai_diff\n",
    "    return np.expand_dims(mat_nt2.dot(mat_nt2.T),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:31:15.415756Z",
     "start_time": "2019-08-02T19:31:15.409413Z"
    }
   },
   "outputs": [],
   "source": [
    "# This version of construct_adiff() is a little simpler, but not (much) faster.\n",
    "def construct_adiff_simpler(j, data_ntk=None):\n",
    "    \"\"\"\n",
    "    Process a single neuron, as specified by 'j'.\n",
    "    \"\"\"\n",
    "    print(f\"Processing neuron {j}\")\n",
    "    if data_ntk is None:\n",
    "        data_ntk = load_data_c_order()\n",
    "\n",
    "    n, t, k = data_ntk.shape # n = 31305, t = 50, k ≈ 800 \n",
    "    jtk = data_ntk[j] # load data from neuron j\n",
    "\n",
    "    # Do all the multiplies in one step.\n",
    "    ai = np.matmul(data_ntk, jtk.T) / k\n",
    "    assert ai.shape == (n,t,t)\n",
    "\n",
    "    mat_nt2 = (ai - ai.transpose(0,2,1)).reshape([n, t**2])\n",
    "    result = mat_nt2.dot(mat_nt2.T)\n",
    "    return result[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:31:15.415756Z",
     "start_time": "2019-08-02T19:31:15.409413Z"
    }
   },
   "outputs": [],
   "source": [
    "def construct_adiff_group_sum(group_start, group_size):\n",
    "    \"\"\"\n",
    "    Process a group of neurons, starting with j=group_start.\n",
    "    \"\"\"\n",
    "    if isinstance(group_start, Iterable):\n",
    "        # You can call this function with an integer or an\n",
    "        # integer wrapped in a list/array, when calling via map_blocks()\n",
    "        assert len(group_start) == 1\n",
    "        group_start = group_start[0]\n",
    "\n",
    "    data_ntk = load_data_c_order()\n",
    "    n, t, k = data_ntk.shape # n = 31305, t = 50, k ≈ 800 \n",
    "    group_end = min(n, group_start+group_size)\n",
    "\n",
    "    result = np.zeros((1,n,n), np.float32)\n",
    "    for j in range(group_start, group_end):\n",
    "        result += construct_adiff_simpler(j, data_ntk)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare the two versions of construct_adiff()\n",
    "#data_ntk = load_data_c_order()\n",
    "#%time r1 = construct_adiff(0, data_ntk)\n",
    "#%time r2 = construct_adiff_simpler(0, data_ntk)\n",
    "#assert r1.shape == r2.shape\n",
    "#assert np.allclose(r1, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:57:09.260945Z",
     "start_time": "2019-08-02T18:57:07.981788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-04 13:33:12,184] INFO mem specification for LSF not set, initializing it to 480000000000 bytes\n"
     ]
    }
   ],
   "source": [
    "TOTAL_ACTIVE_CORES = 1000\n",
    "\n",
    "# On each worker, you can reserve more cores than you plan to actively use,\n",
    "# in case each active CPU needs more than its \"fair share\" of RAM.\n",
    "# The inactive CPUs sit idle, but it's better than running out of RAM.\n",
    "# Even if RAM isn't an issue, it's often a good idea to leave at least one CPU idle,\n",
    "# just to allow background work to run (like garbage collection).\n",
    "\n",
    "# As shown here, I used a 2-1 ratio of 'reserved' cores to 'active' CPUs,\n",
    "# but I think you could get away with 32-to-24 or even 32-to-31.\n",
    "# In the end, the computation didn't need as much RAM as I'd feared.\n",
    "reserved_cores_per_worker = 32\n",
    "active_cores_per_worker = 16\n",
    "\n",
    "# Most of numpy's linear algebra functions such as dot()\n",
    "# and matmul() can use more than one core, if you let them.\n",
    "# If you specify a 2-1 ratio (or more),\n",
    "# we may as well squeeze some value from the 'idle' cores\n",
    "# if we let numpy use them when possible.\n",
    "omp_threads = reserved_cores_per_worker // active_cores_per_worker\n",
    "env_extra = [\n",
    "    f\"export NUM_MKL_THREADS={omp_threads}\",\n",
    "    f\"export OPENBLAS_NUM_THREADS={omp_threads}\",\n",
    "    f\"export OPENMP_NUM_THREADS={omp_threads}\",\n",
    "    f\"export OMP_NUM_THREADS={omp_threads}\",\n",
    "]\n",
    "\n",
    "cluster = get_jobqueue_cluster( walltime=\"12:00\",\n",
    "                                ncpus=reserved_cores_per_worker,\n",
    "                                cores=active_cores_per_worker,\n",
    "                                memory=f\"{32*15}GB\",\n",
    "                                log_directory='worker-logs',\n",
    "                                env_extra=env_extra )\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "workers_needed = int(np.ceil(TOTAL_ACTIVE_CORES / active_cores_per_worker))\n",
    "client.cluster.scale(workers_needed)\n",
    "\n",
    "# Adjust for how many cores we actually requested\n",
    "TOTAL_ACTIVE_CORES = workers_needed * active_cores_per_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:57:09.260945Z",
     "start_time": "2019-08-02T18:57:07.981788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.36.111.13:42011</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.36.111.13:8787/status' target='_blank'>http://10.36.111.13:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>63</li>\n",
       "  <li><b>Cores: </b>1008</li>\n",
       "  <li><b>Memory: </b>30.24 TB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.36.111.13:42011' processes=62 cores=992>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://10.36.111.13:8787/status\n"
     ]
    }
   ],
   "source": [
    "print(client.cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.cluster.close()\n",
    "#client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute with dask\n",
    "Issue all groups in one big computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 31305 neurons in 3131 groups of 10 across 1008 cores\n",
      "CPU times: user 4min 13s, sys: 19.2 s, total: 4min 32s\n",
      "Wall time: 33min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n = 31305 # number of neurons (to determine the size of the result (nn x n x n) which we sum over the first dimension)\n",
    "\n",
    "nn = n\n",
    "#nn = 2000 # subset of neurons to test the code over\n",
    "\n",
    "# Each worker processes one 'group' at a time,\n",
    "# and the data needs to be loaded just once per group.\n",
    "# Therefore bigger groups incur less overhead to read the data.\n",
    "# Furthermore, one (large) intermediate sum needs to be stored per group,\n",
    "# so (in theory) fewer large groups is more RAM efficient than many small groups.\n",
    "# (In fact, if the groups are too small, they use too much RAM and the job fails.)\n",
    "#\n",
    "# However, every worker just gets one task (large groups), we don't get much value out of\n",
    "# dask's progress monitoring.  (It will appear as if there's no progress for a long time,\n",
    "# and then all of the workers start finishing at once.)\n",
    "# For reassurance that *something* is happening, we'll use medium-sized groups.\n",
    "# Even at this setting, you may need to wait for 5-10 minutes before the dask\n",
    "# dashboard shows much activity.\n",
    "GROUP_SIZE = 10 # should be ~3 tasks per worker\n",
    "\n",
    "# Adjust if necessary for test scenarios (when nn is small).\n",
    "GROUP_SIZE = min(nn // TOTAL_ACTIVE_CORES, GROUP_SIZE)\n",
    "GROUP_SIZE = max(1, GROUP_SIZE)\n",
    "\n",
    "num_groups = int(np.ceil(nn / GROUP_SIZE))\n",
    "print(f\"Processing {nn} neurons in {num_groups} groups of {GROUP_SIZE} across {TOTAL_ACTIVE_CORES} cores\")\n",
    "\n",
    "group_starts = da.arange(0, nn, GROUP_SIZE, chunks=1)\n",
    "group_sums = group_starts.map_blocks( construct_adiff_group_sum,\n",
    "                                      group_size=GROUP_SIZE,\n",
    "                                      dtype='float32',\n",
    "                                      new_axis=[1,2],\n",
    "                                      chunks=(1,n,n) ) # bugfix: I changed this from (nn,n,n)\n",
    "final_sum = group_sums.sum(0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/nrs/flyem/bergs/tmp/SCA/final-sum.npy', final_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative execution flow\n",
    "Don't process all neurons in one computation.\n",
    "Use smaller groups, and process the groups in batches.<br>\n",
    "(I thought this might be necessary, or at least make it easier to monitor the job's progress, but now I think it's not better than the single-shot solution above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:31:27.988429Z",
     "start_time": "2019-08-02T19:31:25.732419Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 31305 # number of neurons (to determine the size of the result (nn x n x n) which we sum over the first dimension)\n",
    "\n",
    "nn = n\n",
    "#nn = 10 # subset of neurons to test the code over\n",
    "\n",
    "# Each worker processes one 'group' at a time,\n",
    "# and the data needs to be loaded just once per group.\n",
    "# Therefore bigger groups incur less overhead to read the data.\n",
    "# But smaller group sizes make it easier to monitor progress in the dask dashboard.\n",
    "GROUP_SIZE = 2\n",
    "\n",
    "if nn > TOTAL_ACTIVE_CORES * GROUP_SIZE:\n",
    "    BATCH_SIZE = TOTAL_ACTIVE_CORES * GROUP_SIZE\n",
    "else:\n",
    "    BATCH_SIZE = nn\n",
    "\n",
    "#\n",
    "# The computation requires too much RAM to perform all at once.\n",
    "# Therefore, process the data in smaller batches than the whole set.\n",
    "#\n",
    "final_sum = np.zeros((n,n), np.float32)\n",
    "for batch_number, batch_start in enumerate(range(0, nn, BATCH_SIZE)):\n",
    "    print(f\"Starting batch {batch_number}\")\n",
    "    \n",
    "    group_starts = da.arange(batch_start, batch_start+BATCH_SIZE, GROUP_SIZE, chunks=1)\n",
    "    group_sums = group_starts.map_blocks( construct_adiff_group_sum,\n",
    "                                          group_size=GROUP_SIZE,\n",
    "                                          dtype='float32',\n",
    "                                          new_axis=[1,2],\n",
    "                                          chunks=(1,n,n) ) # bugfix (I think): I changed this from (nn,n,n)\n",
    "    batch_sum = group_sums.sum(0).compute()\n",
    "    final_sum += batch_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/nrs/flyem/bergs/tmp/SCA/final-sum.npy', final_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kill the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:23:51.314945Z",
     "start_time": "2019-08-02T19:23:51.309560Z"
    }
   },
   "outputs": [],
   "source": [
    "client.cluster.stop_all_jobs()\n",
    "client.cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
